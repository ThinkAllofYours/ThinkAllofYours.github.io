---
slug: basic_tensor
title: 🚀 텐서 기본
authors: [bundabergman]
tags: [ai, basic_tensor]
---

# 텐서 기본

## 텐서 기본

일단 Tensor의 기본 사용법을 배우려면 
알고리즘이 어떤 구조로 수행되는지 알 수 있어야한다. 

:::tip 💡 참고
아래를 작성해보라

일단 주석만 보고 어떤 코드가 들어가는지 예상해보라
:::

```python
# 시드 고정: 재현성을 위해 난수 생성 결과를 동일하게 설정
# 입력 데이터 생성: 정규분포(평균 0, 표준편차 1)을 따라는 난수 텐서 (샘플 1개, 특성 3개)
# 신경망의 계층 크기 정의

# 모델 구성: 은닉층과 출력층 뉴런 수 설정
# 은닉층 뉴런 수
# 출력층 뉴런 수 (예: 회귀 문제)

# 가중치 및 편향 초기화
# 입력층 -> 은닉층 가중치 ( 3 x 2 )
# 은닉층 편향 ( 2 X 1 )
# 은닉층 -> 출력층 가중치 ( 2 x 1 )
# 출력층 편향 ( 1 x 1 )

# 순전파 계산
# 입력층에서 은닉층 계산
# 은닉층에서 출력층 계산

# 최종 예측 결과 출력
```

```python
def activation (x):
    return 1 / (1 + torch.exp(-x))
```


- 샘플 1개, 특성 3개은 무엇을 말하는가?

샘플1개 : 신경망 모델에 입력되는 데이터 하나를 의미함 (예를 들어 이미지 분류 모델이라면 이미지 한 장이 샘플이
될 수 있습니다. )

특성3개 : 사람이 샘플 1개라면 특성 3개는 사람의 키, 몸무게, 나이 와 같은것

- 샘플 1개, 특성 3개라면 은닉층과 출력층을 어떻게 설정해야할까?

은닉층은 2개 : n_hidden = 2
출력층은 1개 : n_output = 1


```python
# 시드 고정: 재현성을 위해 난수 생성 결과를 동일하게 설정
torch.manual_seed(7)
# 입력 데이터 생성: 정규분포(평균 0, 표준편차 1)을 따라는 난수 텐서 (샘플 1개, 특성 3개)
features = torch.rand((1,3))
# 신경망의 계층 크기 정의
n_input = features.shape[1]
# 모델 구성: 은닉층과 출력층 뉴런 수 설정
# 은닉층 뉴런 수
n_hidden = 2
# 출력층 뉴런 수 (예: 회귀 문제)
n_ouput = 1

# 가중치 및 편향 초기화
# 입력층 -> 은닉층 가중치 ( 3 x 2 )
W1 = torch.randn(n_input, n_hidden)
# 은닉층 편향 ( 2 X 1 )
W2 = torch.randn(n_hidden)
# 은닉층 -> 출력층 가중치 ( 2 x 1 )
B1 = torch.randn((1, n_hidden))
# 출력층 편향 ( 1 x 1 )
B2 = torch.randn((1, n_output))

# 순전파 계산
# 입력층에서 은닉층 계산
h = activation(torch.mm(features, W1) + B1)

# 은닉층에서 출력층 계산
output = activation(torch.mm(h, W2) + B2)

# 최종 예측 결과 출력
print(output)
```

## 정리
## 코드 요약 및 설명

## 1. 시드 고정 (seed 고정)
- `torch.manual_seed(7)`
- **목적**: 난수 생성 시 항상 동일한 결과가 나오도록 설정하여, 코드 실행 결과의 재현성을 확보합니다.
- 딥러닝 모델의 학습은 무작위성에 의존하므로, 실험 결과를 재현하고 비교하기 위해 시드 고정이 필수적입니다.

## 2. 입력 데이터 생성
- `features = torch.rand((1,3))`
- **역할**: 모델에 입력될 데이터를 생성합니다.
- **형태**: 1개의 샘플과 3개의 특성을 가진 텐서 형태입니다.
- `torch.rand` 함수는 0과 1 사이의 무작위 값을 생성합니다.

## 3. 신경망 구조 정의
- `n_input = features.shape[1]`
- `n_hidden = 2`
- `n_output = 1`
- **의미**:*
    - `n_input`: 입력층의 노드 수 (특성 개수)
    - `n_hidden`: 은닉층의 노드 수
    - `n_output`: 출력층의 노드 수 (예측하려는 값의 개수)

## 4. 가중치 및 편향 초기화
- `W1 = torch.randn(n_input, n_hidden)`
- `W2 = torch.randn(n_hidden)`
- `B1 = torch.randn((1, n_hidden))`
- `B2 = torch.randn((1, n_output))`
- **역할**:*
    - `W1`: 입력층에서 은닉층으로 연결되는 가중치
    - `W2`: 은닉층에서 출력층으로 연결되는 가중치
    - `B1`: 은닉층의 편향
    - `B2`: 출력층의 편향
- `torch.randn` 함수는 평균이 0이고 표준편차가 1인 정규분포를 따르는 난수를 생성합니다.

## 5. 순전파 계산
- `h = activation(torch.mm(features, W1) + B1)`
- `output = activation(torch.mm(h, W2) + B2)`
- **설명**:*
    - `torch.mm`: 행렬 곱셈 연산
    - `activation`: 활성화 함수 (sigmoid, ReLU 등)
    - 입력 데이터 -> 은닉층 -> 출력층 순서로 계산을 진행합니다.

## 6. 최종 결과 출력
- `print(output)`
- **역할**: 모델의 최종 예측 결과를 화면에 출력합니다.

## 추가 설명

- 이 코드는 **단층 퍼셉트론** 또는 **2계층 신경망**을 구현한 것으로 볼 수 있습니다.
- 활성화 함수(`activation`)는 코드에 명시되어 있지 않지만, 일반적으로 sigmoid, ReLU 등의 함수가 사용됩니다.
- 이 코드는 단순한 형태의 신경망으로, 실제 문제에 적용하기 위해서는 모델 구조, 활성화 함수, 손실 함수, 최적화 알고리즘 등을 추가해야 합니다.

궁금한 점이 있다면 언제든지 질문해주세요.


## 자 이제 주석 없이 한번 쭉 써봅시다.

```python
import torch

def activation(x):
    return 1 / (1 + torch.exp(-x))

torch.manual_seed(7)
features = torch.randn((1,3))
n_input = features.shape[1] # 3
n_hidden = 2
n_output = 1

W1 = torch.randn(n_input, n_hidden) # 3, 2
W2 = torch.randn(n_hidden, n_output) # 2, 1
B1 = torch.randn(1, n_hidden) # 1, 2
B2 = torch.randn(1, n_output) # 1, 1

h = activation(torch.mm(features, W1) + B1)
output = activation(torch.mm(h, W2) + B2)

print(output)
```