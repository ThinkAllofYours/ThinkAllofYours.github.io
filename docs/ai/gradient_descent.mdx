---
slug: gradient_descent
title: ğŸš€ Gradient Descent
authors: [bundabergman]
tags: [ai, fundamentals]
---

## ê²½ì‚¬ í•˜ê°•ë²•ì´ë€?

ê²½ì‚¬ í•˜ê°•ë²•ì€ ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ìµœì í™” ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ì¼ë°˜ì ì¸ ê¸°ìˆ 

### ê²½ì‚¬ í•˜ê°•ë²•ì˜ ê¸°ë³¸ ê°œë…

ê²½ì‚¬ í•˜ê°•ë²•ì€ ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ì—¬ ìµœì†Œí™”í•˜ëŠ” ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤. 
ì´ ê¸°ìˆ ì€ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì—¬ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê³¼ì •ì„ í¬í•¨í•©ë‹ˆë‹¤.

### ê²½ì‚¬ í•˜ê°•ë²•ì˜ ìˆ˜ì‹

ê²½ì‚¬ í•˜ê°•ë²•ì˜ ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

$$
\theta = \theta - \eta \nabla J(\theta)
$$

ì—¬ê¸°ì„œ $\theta$ëŠ” ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°, $\eta$ëŠ” í•™ìŠµë¥ , $\nabla J(\theta)$ëŠ” ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ì…ë‹ˆë‹¤.

### ê²½ì‚¬ í•˜ê°•ë²• python ì½”ë“œ ì˜ˆì‹œ

```python
import numpy as np

# í™œì„±í™” í•¨ìˆ˜ë¡œ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ ì •ì˜í•˜ê¸°
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì˜ ë„í•¨ìˆ˜
def sigmoid_prime(x):
    return sigmoid(x) * (1 - sigmoid(x))

# ì˜ˆì‹œ ì…ë ¥ê°’
X = np.array([0.5, 0.8])
# ëª©í‘œê°’
y = 0.2
# ì¶œë ¥ ê°€ì¤‘ì¹˜ì˜ ì…ë ¥
np.random.seed(42)
weights = np.random.rand(2)

# í•™ìŠµë¥ 
learning_rate = 0.5

# ì‹ ê²½ë§ ì¶œë ¥ (y-hat)
h = np.dot(X, weights)
nn_output = sigmoid(h)

# ì¶œë ¥ ì˜¤ì°¨ (y - y-hat)
error = y - nn_output

# ì¶œë ¥ ê¸°ìš¸ê¸° (ì˜¤ì°¨ê°€ ê¸°ìš¸ê¸°ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ sigmoidë¥¼ ë¯¸ë¶„í•˜ì—¬ ê³„ì‚°)
output_gradient = error * sigmoid_prime(h)

# ê²½ì‚¬ í•˜ê°•ë²• 
del_w = learning_rate * output_gradient * X

# ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
weights += del_w

print(weights)
```
