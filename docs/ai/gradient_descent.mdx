---
slug: gradient_descent
title: 🚀 Gradient Descent
authors: [bundabergman]
tags: [ai, fundamentals]
---

## 경사 하강법이란?

경사 하강법은 머신러닝에서 최적화 문제를 해결하는 데 사용되는 일반적인 기술

### 경사 하강법의 기본 개념

경사 하강법은 손실 함수의 기울기를 계산하여 최소화하는 방법을 제공합니다. 
이 기술은 머신러닝 모델의 파라미터를 조정하여 손실 함수를 최소화하는 과정을 포함합니다.

### 경사 하강법의 수식

경사 하강법의 수식은 다음과 같습니다.

$$
\theta = \theta - \eta \nabla J(\theta)
$$

여기서 $\theta$는 모델의 파라미터, $\eta$는 학습률, $\nabla J(\theta)$는 손실 함수의 기울기입니다.

### 경사 하강법 python 코드 예시

```python
import numpy as np

# 활성화 함수로 시그모이드 함수 정의하기
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# 시그모이드 함수의 도함수
def sigmoid_prime(x):
    return sigmoid(x) * (1 - sigmoid(x))

# 예시 입력값
X = np.array([0.5, 0.8])
# 목표값
y = 0.2
# 출력 가중치의 입력
np.random.seed(42)
weights = np.random.rand(2)

# 학습률
learning_rate = 0.5

# 신경망 출력 (y-hat)
h = np.dot(X, weights)
nn_output = sigmoid(h)

# 출력 오차 (y - y-hat)
error = y - nn_output

# 출력 기울기 (오차가 기울기에 미치는 영향 sigmoid를 미분하여 계산)
output_gradient = error * sigmoid_prime(h)

# 경사 하강법 
del_w = learning_rate * output_gradient * X

# 가중치 업데이트
weights += del_w

print(weights)
```
